\documentclass[12pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{setspace}
\author{Jesus Gonzalez, Tomas McCandless}
\title{CS 388P Writeup}
\date{October 9, 2012}
\begin{document}
\maketitle
\hyphenpenalty=1000
\doublespace
\newpage

% k = max contention

\section{Summary of Contributions}

\subsection{Emulations between QSM, BSP and LogP: A framework for general-purpose parallel algorithm design. \cite{Vlr03}}
The authors present work-preserving emulations with just a small slowdown between the geneal-purpose models: LogP, BSP and QSM. The relevance of this lies on the simplicity of the QSM model, which makes it easy to program for and, yet, through emulation on more realistic models like BSP and LogP run an algorithm with just a logarithmic slowdown.

The authors present 3 algorithms for basic problems (prefix sums, sample sort and list ranking). All of them are optimal and simulations show that the predicted performance for each of them very close, with just insignificant discrepancy.

\section{Summary of Key Papers}
\subsection{Can a Shared-Memory Model Serve as a Bridging Model for Parallel Computation?}
A bridging model for parallel computation is a model that ``spans the range from algorithm design to architecture to hardware". \cite{Gib99}
Such a model should provide an abstraction that is easy for algorithm designers to use, and should be realizable by hardware and system sofware at varying
price/performance points.

Models of parallel computation should attempt to satisfy two goals that are in conflict: a model should be sufficiently abstract to allow algorithm designers to
write programs that are simple and portable across architectures, and a model should also expose some low-level architectural details to allow for optimization. 
The PRAM model is both widely used and simple, yet it has been criticized for being too high-level and thus failing to accurately model parallel machines.
Specifically, the PRAM does not model realities of current parallel machines, such as bandwidth limitations. Similarly, network-based models such as the
hypercube have been criticized for being too low-level, failing to be widely reflective of the current technological state of parallel machines. 

These shortcomings of existing models have led to the introduction of more intermediate models such as the BSP or LogP models.

The new model this paper introduces, the QSM, is meant to be an attractive candidate for a bridging model. The QSM consists of processors with private
individual memory in addition to a global shared memory.

One goal for a 
higher-level shared-memory model should be allowing efficient emulation on lower-level models which may be more realistic. 

It is mistaken to view ``shared-memory model" as being synonymous with PRAM. The shared-memory abstraction refers to interprocessor communication. That is, a
processor may have additional memory as part of its local state. While the PRAM does not model bandwidth limitations, a shared-memory model may be
asynchronous, or have explicit charges for communication. 

One of the primary motivations for positing a shared-memory model as a bridging model is the portability across memory architectures that accompanies such
models. This is because the layout of memory is hidden by the model.

The QSM model consists of a number of processors, each with private memory, that communicate via reading and writing to shared memory locations. Each
synchronized phase executed by the processors consists of an arbitrary interleaving of the following operations:

\begin{itemize}
\item shared-memory reads: processor $P_i$ copies values from shared memory locations into private memory. 
\item shared-memory writes: processor $P_i$ writes to $w_i$ shared memory locations.
\item local computation: processor $P_i$ performs $c_i$ RAM computations which involve only private state and private memory.
\end{itemize}

The QSM has the following features which make it more attractive as a bridging model:

\begin{itemize}
\item shared-memory abstraction
\item bulk-synchrony
\item few parameters
\item queue contention metric
\item work-preserving emulation on BSP
\item work-preserving emulation of BSP
\end{itemize}

The authors compare the QSM and the BSP in terms of their effectiveness as bridging models for parallel computation. The BSP consists of $p$ processor/memory
components that communicate by sending point-to-point messages. The network supporting this communication is by two parameters: $g$, which models bandwidth;
and $L$, which models latency. A computation on the BSP is a series of supersteps, each of which is separated by bulk synchronization.

The authors give a work preserving emulation (an emulation is work-preserving if both models perform the same amount of work to within a constant factor) of
the QSM on the BSP. However, since the QSM has fewer parameters than the BSP and does not deal directly with memory partitioning, we would expect it to be
easier to design algorithms on the QSM. Designs can then be mapped onto the BSP with modest slowdown. 

The ($d$, $\textbf{x}$)-BSP is more detailed in its modeling of memory contention than the BSP, and is characterized by 5 parameters:

\begin{itemize}
\item $p$ - number of processors
\item $g$ - bandwidth
\item $L$ - latency
\item $d$ - delay, models gap at the memory banks
\item $\textbf{x}$ - expansion, ratio of memory banks to processors
\end{itemize}

Developing a suitable model for parallel computation at a proper level of abstraction has remained a challenging problem for computer scientists. BSP and LogP
have recently gained popularity as candidate bridging models. The QSM is an attractive candidate for a bridging model, in particular because it provides a
simple shared memory abstraction and has a small number of parameters ($p$, the number of processors, and $g$, the bandwidth gap). Additionally, the QSM can be
efficiently emulated on both the BSP and ($d$, $\textbf{x}$)-BSP.


\subsection{Emulations between QSM, BSP and LogP: A framework for general-purpose parallel algorithm design. \cite{Vlr03}}

PRAM is a very simple model for parallel computer that abstracts too much parameters from real machines, which turns into apparently eficcient models that actually run slow on real machines. Therefore, exists the need of more realistic models that reflect with more fidelity a machine and yet is not to complicated so that implementation is kept simple and portable. BSP and LogP are models that abstract more acurately than the PRAM model a machine, but there is a simple model, QSM, which could be used instead, since it is posibble to efficiently emulate it on LogP and BSP.
\subsubsection{Brief description of the models}
Al the next models have work measured as the time-processors product.
\paragraph{BSP}
BSP (Bulk-Synchronous Parallel) takes 3 parameters:
\begin{itemize}
	\item p processor/memory components: Components that communicate via point-to-point messages.
	\item Bandwidth g.
	\item Latency L.
\end{itemize}
A computation in this model consists of a sequence of supersteps, each separated by a bulk synchronization. In each superstep, a component can perform local computations, send and receive a set of messages. The cost \textit T superstep is calculated as $T = max(w, gh, L)]$ and the overal time that takes an algorithm to run is the sum of T of every superstep.
\paragraph{LogP}
LogP (Latency, overhead, gap, Processors) takes 4 parameters and a derived value:
\begin{itemize}
	\item Processors p: Number of processors.
	\item Latency l: Time taken by network to transmit a message from one processor to another is at most l.
	\item Gap g: Least units of time taken between sending or reception of a message by a processor.
	\item Overhead o: Time taken by a processor to send or receive a message.
	\item Capacity constraint: Maximum number of messagings heading to the same processor can't be larger than $\lceil l/g \rceil$
\end{itemize}
Once a processor has reached its capacity, another processor trying to send a message will stall. The time taken by a LogP algorithm is the longest time taken by any processor to end its operation.
\paragraph{QSM}
QSM (Queuing Shared Memory) consists of p processors that communicate through shared memory, but run computations on private memory. An algorithm consists of the analogous for a superstep on BSP, a synchronized phase. Concurrent reads and concurrent writes are permited on a single synchronized phase over the same shared-memory location, but not both.
Metrics:
\begin{itemize}
	\item Processors p.
	\item Maximum contention k: of a synchronized phase Largest number of processors reading or writing a location.
\end{itemize}
The time cost of a phase is $max(m_{op}, gm_{rw}, k)$, where $m_{op}= $ maximum operations performed by any processor and $m_{rw}=$maximum read/write operations performed by any processor and $k=$ \textit{maximum contention}. The time taken by an algorithm is the sum of the cost of all of its phases.
s-QSM is a variation of QSM where the time cost of a phase is $max(m_{op}, gm_{rw}, gk)$

\subsubsection{Emulations}
The paper, in addition with results obtained in other two papers, show emulations for: LogP, s-QSM and QSM on BSP. BSP, sQSM and QSM on LogP. BSP and LogP on S-QSM. BSP, LogP and s-QSM on QSM. All of them are work-preserving emulations.

\subsubsection{Algorithms}
In this section, the authors present optimal algorithms for prefix sums, sample sort and list ranking on QSM. This way, they show that QSM may be considered a general-purpose model. They also present a particular cost metric for QSM algorithms: number of phases; the authors claim that the time spent on synchronization is proportional to the number of phases, and, since QSM doesn't takes directly into account lattency and synchronization cost, it should be considered indirectly with this metric. All the algorithms they present have a number of phases independent to the size of the input (See open problems).

\subsubsection{Experimental results}
As stated previously, emulations between all the models are work-preserving, which supports the use of QSM as a model. The estimated costs of emulating QSM are confirmed by experiments run on Armadillo, a framework for simulating multiprocessor machines, but with a discrepancy on the predicted and actual communication cost, which occurs to be irrelevant compared with the total computation cost with large problems.

\newpage
\section{Open Problems}

The main problem that parallel models are currently facing is: Is there a method that can automatically optimize the code generated for a model so that the hidden parameters become irrelevant for any algorithm? Is communication sufficiently optimizable or at least irrelevant for any algorithm given a large problem size? Could any problem be solved in few phases, refering to QSM.

A related question arise from the results of {Vlr03}: Would emulations correctly predict the total cost for algorithms with high level of communication? Will the emulations still be valid for machines with massive number of processors? Neural network simulation, for example, is naturally benefited from a massive number of processors and heavily relies on communication between them. Of course it is a radical example and reducing the number of phases could be very challenging, if not impossible at all, but it shows that a method to analize the nature of a problem is needed in order to know if QSM is applicable.

\newpage
\singlespace
\begin{thebibliography}{9}

\bibitem[1]{Gib99} P. Gibbons, Y. Matias, V. Ramachandran. ``Can a shared memory model serve as a bridging model for parallel computation?" Theory of Computing Systems Special Issue on papers from SPAA'97, vol. 32, no. 3, 1999, pp. 327-359
\bibitem[2]{Vlr03} V. Ramachandran, B.Grayson, M. Dahlin.(2003) ``Emulations between QSM, BSP and LogP: A framework for general-purpose parallel algorithm design." Journal of Parallel and Distributed Computing, vol. 63, 2003, pp. 1175-1192. 
\end{thebibliography}

\end{document}
